# Cranfield Search Engine

This is a simple information retrieval (IR) system that implements three different retrieval models for searching the Cranfield dataset.

## Features

- Text preprocessing (tokenization, stopword removal, stemming)
- Inverted index creation
- Three retrieval models:
  - Vector Space Model (TF-IDF)
  - BM25
  - Language Model with Dirichlet smoothing
- Results output in TREC format for evaluation

## Project Structure

```
search-engine/
├── src/
│   ├── preprocessor.py  # Text preprocessing utilities
│   ├── indexer.py       # Inverted index creation
│   ├── search.py        # Retrieval models implementation
│   ├── parser.py        # Dataset parser
│   ├── trec_writer.py   # TREC format output writer
│   └── main.py          # Main program
├── output/              # Output directory for search results
├── start.py             # All-in-one startup script
├── run_search.sh        # Shell script to run the search engine
├── evaluate.py          # Script to evaluate results with trec_eval
├── test_imports.py      # Script to test imports
├── check_dataset.py     # Script to check dataset files
├── requirements.txt     # Python dependencies
└── README.md            # Project documentation
```

## Requirements

- Python 3.6+
- NLTK
- XML parsing libraries

## Installation

1. Install Python dependencies:

```bash
pip3 install -r requirements.txt
```

2. Make sure you have the Cranfield dataset in TREC XML format:
   - Documents: `cran.all.1400.xml`
   - Queries: `cran.qry.xml`
   - Relevance judgments (optional for evaluation): `cranqrel.trec.txt`

## Quick Start

The easiest way to run the system is to use the all-in-one startup script:

```bash
python3 start.py
```

This script will:
1. Check if all required imports are available
2. Verify that the dataset files exist
3. Run the search engine if all checks pass

If you prefer to run the steps individually:

```bash
# Check if required imports are available
python3 test_imports.py

# Check if dataset files exist
python3 check_dataset.py

# Run the search engine
./run_search.sh
```

## Manual Usage

If you prefer to run commands manually:

```bash
# Run the search engine
python3 src/main.py --documents <path_to_documents> --queries <path_to_queries> --output_dir output --run_id my_run

# Evaluate results
python3 evaluate.py --qrels <path_to_qrels> --results_dir output
```

## Retrieval Models

### Vector Space Model (TF-IDF)

The Vector Space Model represents documents and queries as vectors in a high-dimensional space where each dimension corresponds to a term. TF-IDF weighting is used, and document ranking is based on cosine similarity.

### BM25

BM25 is a probabilistic retrieval model that extends the Binary Independence Model with term frequency normalization and document length normalization.

### Language Model with Dirichlet Smoothing

This model ranks documents based on the probability that a query is generated by the document's language model. Dirichlet smoothing is used to address the sparsity problem.

## Evaluation

You can evaluate the results using the `trec_eval` tool:

```bash
trec_eval <path_to_qrels> output/results_vsm.txt
trec_eval <path_to_qrels> output/results_bm25.txt
trec_eval <path_to_qrels> output/results_lm_dirichlet.txt
```

## Credits

- Cranfield dataset: A standard test collection for information retrieval research
- NLTK: Used for text processing and stemming 